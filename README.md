# COMP34812CW

## 1. Overview

This project implements a transformer-based solution (Category C) for the COMP34812 shared task on Natural Language Inference (NLI). The model is based on `microsoft/deberta-v3-base`, a pre-trained language model that extends BERT with disentangled attention and an enhanced decoding mechanism.

To improve efficiency and reduce the number of trainable parameters, we apply LoRA (Low-Rank Adaptation) via the PEFT (Parameter-Efficient Fine-Tuning) framework. This approach allows us to fine-tune only a small number of added parameters, while keeping the core DeBERTa weights frozen.

We also integrate Optuna to automatically search for optimal hyperparameters based on F1-score on the validation set. The best-performing configuration was then used to retrain and evaluate the model.

The final model was evaluated on the full development set (dev.csv) provided in the coursework. All predictions, logs, and saved models are included in this submission.

---

## 2. File Structure

*Due to the GitHub's file size limit, the full best model is stored in google drive: https://drive.google.com/file/d/187MRDqhL-R8TtLHdwEGhKIAdBZnTL8rL/view?usp=drive_link*


.
├── comp34812-cw-c.ipynb          # Trains the DeBERTa model with LoRA and generates predictions on dev.csv
│
├── predictions.csv               # Final prediction output on dev set for Codabench
├── train_logs.csv                # Per-epoch training metrics (loss, accuracy, F1)
├── val_logs.csv                  # Per-epoch validation metrics (loss, accuracy, F1)
├── loss_curve.png                # Train vs validation loss plot
├── acc_f1_curve.png              # Validation accuracy and F1 score plot
├── my_model_card.md              # Model description, metadata and evaluation results
│
├── demo/
│   ├── comp34812-cw-c-demo.ipynb # Demo notebook for inference on new test data
│   └── Group_48_C.csv            # Prediction file submitted to Blackboard

---

## 3. Requirements

This project requires Python 3.11 or later and the following Python libraries:

- transformers
- datasets
- peft
- accelerate
- scikit-learn
- optuna
- matplotlib
- torch

You can install all dependencies locally using the following command:

```bash
pip install --upgrade transformers datasets peft accelerate scikit-learn optuna matplotlib torch
```

---

## 4. Running the Code

### 4.1 Running in Kaggle
Both the training and demo notebooks were implemented and executed in the Kaggle environment:

- **Training Notebook**: [comp34812-cw-c](https://www.kaggle.com/code/jason1768/comp34812-cw-c)  
- **Demo Notebook**: [comp34812-cw-c-demo](https://www.kaggle.com/code/jason1768/comp34812-cw-c-demo)

All required input files (e.g., `train.csv`, `dev.csv`) are already located in the appropriate Kaggle directory paths.

To run the notebooks:

1. Click the **"Copy & Edit"** button in the top right corner of the notebook page to duplicate the notebook into your workspace.
2. Once opened, click the **"Run All"** button from the menu bar to execute all cells sequentially.

> If a **network error or kernel disconnection** occurs, click the **"Factory Reset Runtime"** button (top right), then click **"Run All"** again to restart from a clean environment.

There is **no need to install any additional dependencies manually**. All required packages are automatically installed at the beginning of each notebook.


### 4.2 Running Locally

To run the notebooks locally, please follow these steps:

1. Install all required packages as described in **Section 3 (Requirements)**.
2. Manually download the input files (`train.csv`, `dev.csv`, `test.csv`) from Kaggle and place them in your local working directory.
3. Open the notebooks and modify the input/output file paths where necessary to match your local folder structure.

> **Note:** Running the notebooks in Kaggle is still highly recommended for simplicity.  
> Kaggle does not support downloading entire input folders, so each file must be downloaded separately when working locally.

---

## 5. Saved Model and Output Files
### 5.1 Saved Model

Only one version of the trained model is retained:

- `best_model_full.zip`  
  This compressed file contains the fully merged version of the DeBERTa base model and LoRA adapter.  
  It can be loaded **directly** using HuggingFace Transformers without requiring the PEFT library.

> Note: The original PEFT-format model (`best_model/`) has been commented out to simplify file management.  
> The **demo notebook requires this merged model** and will not work with PEFT-only weights.

You can download the trained model from Google Drive:  
[Download zipped merged model](https://drive.google.com/file/d/187MRDqhL-R8TtLHdwEGhKIAdBZnTL8rL/view?usp=drive_link)


### 5.2 Prediction Outputs

- `predictions.csv`  
  Generated by the **training notebook** (`comp34812-cw-c.ipynb`) using `dev.csv` as input.  
  This file is used for **Codabench evaluation**.

- `demo/Group_48_C.csv`  
  Generated by the **demo notebook** (`comp34812-cw-c-demo.ipynb`) on a separate test input file.  
  This file is submitted to **Blackboard**.

### 5.3 Log Files and Performance Plots

- `train_logs.csv`, `val_logs.csv`  
  Contain per-epoch metrics (loss, accuracy, F1-score) from training and validation.

- `loss_curve.png`  
  Visualizes training and validation loss over time.

- `acc_f1_curve.png`  
  Shows validation accuracy and F1-score across epochs.

---

## 6. Attribution

- **Base model**: This project uses [`microsoft/deberta-v3-base`](https://huggingface.co/microsoft/deberta-v3-base), a pre-trained transformer model published by Microsoft.

- **Training data**: All training and evaluation data used in this project (`train.csv`, `dev.csv`, `test.csv`) were provided via Blackboard.

- **Libraries and tools**:
  - [Transformers](https://github.com/huggingface/transformers) (HuggingFace)
  - [PEFT](https://github.com/huggingface/peft) for LoRA fine-tuning
  - [Datasets](https://github.com/huggingface/datasets) (HuggingFace)
  - [Optuna](https://optuna.org/) for hyperparameter optimization
  - [PyTorch](https://pytorch.org/) as the deep learning backend
  - [Scikit-learn](https://scikit-learn.org/) for metrics
  - [Matplotlib](https://matplotlib.org/) for training visualizations

- No external datasets or fine-tuned models outside of the coursework specification were used.

- The final model and output predictions were generated solely using our own training process.

